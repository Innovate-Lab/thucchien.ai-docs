---
sidebar_position: 7
---

# Tham khảo

Để hiểu sâu hơn về các mô hình và công nghệ được sử dụng trong hệ thống này, bạn có thể tham khảo các tài liệu chính thức từ các nhà cung cấp.

## LiteLLM

- **[Tài liệu chính thức của LiteLLM](https://docs.litellm.ai/docs/)**: Nguồn thông tin toàn diện nhất về cách cài đặt, cấu hình và sử dụng LiteLLM Proxy, bao gồm các tính năng nâng cao như caching, fallbacks, và virtual keys.
- **[Hỗ trợ API của OpenAI trên LiteLLM](https://docs.litellm.ai/docs/providers/openai)**: Chi tiết về cách LiteLLM triển khai và hỗ trợ các endpoint theo chuẩn OpenAI.

## Google

- **[Tài liệu Google AI Gemini API](https://ai.google.dev/docs/gemini_api_overview)**: Tài liệu chính thức từ Google về cách sử dụng các mô hình Gemini, bao gồm các tham số chi tiết và các khả năng của mô hình.
- **[Tài liệu Vertex AI](https://cloud.google.com/vertex-ai/docs)**: Dành cho các mô hình được host trên nền tảng Vertex AI như Imagen và Veo.

## Azure OpenAI

- **[Tài liệu Azure OpenAI Service](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/)**: Hướng dẫn chi tiết về cách triển khai và sử dụng các mô hình của OpenAI trên nền tảng Microsoft Azure.

Việc tham khảo các tài liệu này sẽ giúp bạn tận dụng tối đa sức mạnh của từng mô hình và giải quyết các vấn đề phức tạp hơn.
